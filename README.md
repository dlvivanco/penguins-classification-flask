# ğŸ§ ClasificaciÃ³n de PingÃ¼inos del ArchipiÃ©lago Palmer

Este proyecto implementa una soluciÃ³n completa de *Machine Learning* para predecir la especie de un pingÃ¼ino basÃ¡ndose en sus medidas fÃ­sicas. El flujo de trabajo abarca desde el anÃ¡lisis exploratorio de datos hasta el despliegue de modelos mediante una API REST con Flask.


## ğŸ“‹ DescripciÃ³n del Proyecto

El objetivo es clasificar pingÃ¼inos en tres especies (**Adelie**, **Chinstrap**, **Gentoo**) utilizando el dataset **Palmer Archipelago (Antarctica) penguin data**.

El proyecto incluye:

- âœ… **PreparaciÃ³n de datos**: eliminaciÃ³n de filas con `NA`, codificaciÃ³n *one-hot* con `DictVectorizer` y normalizaciÃ³n con `StandardScaler`.
- âœ… **Entrenamiento y evaluaciÃ³n** de 4 modelos: **RegresiÃ³n LogÃ­stica**, **SVM**, **Ãrbol de DecisiÃ³n**, **KNN**.
- âœ… **SerializaciÃ³n** de los modelos (`.pck`) para reutilizarlos en despliegue.
- âœ… **API REST con Flask** para realizar predicciones.
- âœ… **Cliente** para enviar al menos 2 peticiones a cada modelo y mostrar las respuestas.
---

## ğŸ“‚ Estructura del Repositorio

```plaintext
penguins-classification-flask/
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ penguins_size.csv       # Dataset original (requerido)
â”‚
â”œâ”€â”€ models/                     # Modelos entrenados (.pck)
â”‚   â”œâ”€â”€ decision_tree.pck
â”‚   â”œâ”€â”€ knn.pck
â”‚   â”œâ”€â”€ logistic_regression.pck
â”‚   â””â”€â”€ svm.pck
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ train_model.ipynb       # Notebook de limpieza y entrenamiento
â”‚   â””â”€â”€ client.ipynb            # Notebook para lanzar servidor y probar clientes
â”‚
â”œâ”€â”€ predict_app.py              # Script del servidor Flask (generado)
â”œâ”€â”€ requirements.txt            # Dependencias del proyecto
â””â”€â”€ README.md                   # DocumentaciÃ³n del proyecto

```

---

## ğŸš€ InstalaciÃ³n y requisitos

Requisitos: **Python 3.9+**

Archivo llamado requirements.txt en la carpeta raÃ­z del proyecto con el siguiente contenido:

pandas
numpy
scikit-learn
flask
requests
seaborn
matplotlib
jupyter

Puedes instalar todas estas dependencias de golpe ejecutando el siguiente comando en tu terminal (asegÃºrate de tener tu entorno virtual activado):

### OpciÃ³n A: usar `pip` (rÃ¡pida)

```bash
pip install -r requirements.txt
```

### OpciÃ³n B: usar Conda (recomendado)

```bash
conda create -n penguins_env python=3.9
conda activate penguins_env
pip install -r requirements.txt
```

---

## ğŸ“¦ Dataset

- `dataset/penguins_size.csv`


---

## ğŸ§ª Entrenamiento y serializaciÃ³n de modelos

Ejecuta paso por paso el Jupyter Notebook:

```bash
notebooks/train_model.ipynb
```

Esto:

1) Carga el dataset y elimina filas con NA  
2) Divide datos **80% train / 20% test**  
3) Escala numÃ©ricas con `StandardScaler` (fit en train, transform en train/test)  
4) One-hot de categÃ³ricas con `DictVectorizer`  
5) Entrena y evalÃºa 4 modelos  
6) Guarda 4 ficheros en `models/`:

- `models/logistic_regression.pck`
- `models/svm.pck`
- `models/decision_tree.pck`
- `models/knn.pck`

---

## ğŸŒ Ejecutar la API (Flask)

Arranca el servidor:

```bash
python predict_app.py
```

El servidor corre en:

- `http://127.0.0.1:9696`

### Endpoints

La API expone un endpoint por modelo:

- `POST /predict/lr`
- `POST /predict/svm`
- `POST /predict/dt`
- `POST /predict/knn`


### Ejemplo de peticiÃ³n (JSON)

```json
{
  "island": "Torgersen",
  "culmen_length_mm": 39.1,
  "culmen_depth_mm": 18.7,
  "flipper_length_mm": 181.0,
  "body_mass_g": 3750.0,
  "sex": "MALE"
}
```

### Ejemplo con `curl`

```bash
curl -X POST "http://127.0.0.1:9696/predict/lr"   -H "Content-Type: application/json"   -d '{
    "island":"Torgersen",
    "culmen_length_mm":39.1,
    "culmen_depth_mm":18.7,
    "flipper_length_mm":181.0,
    "body_mass_g":3750.0,
    "sex":"MALE"
  }'
```

---

## ğŸ§‘â€ğŸ’» Cliente de prueba

Con el servidor encendido, ejecutamos paso por paso el Jupyter Notebook::

```bash
notebooks/client.ipynb
```

El cliente realiza **al menos 2 peticiones por modelo** y muestra las respuestas en consola.

---

## ğŸ§  Modelos implementados

- **RegresiÃ³n logÃ­stica**: baseline lineal
- **SVM**: hiperplano separador (en el ejemplo con `probability=True`)
- **Ãrbol de decisiÃ³n**: reglas con profundidad acotada para evitar sobreajuste
- **KNN**: clasificaciÃ³n por vecinos mÃ¡s cercanos

---

## ğŸ“Š Preprocesamiento aplicado

- **Limpieza**: eliminaciÃ³n de filas con valores nulos (`dropna`).
- **Escalado**: `StandardScaler` sobre variables numÃ©ricas (media 0, desviaciÃ³n 1).  
  - Se ajusta **solo con train** y se aplica a **train y test**.
- **CodificaciÃ³n**: `DictVectorizer` para variables categÃ³ricas (`island`, `sex`) mediante one-hot.

---

## âœ… Checklist de entrega (rÃºbrica)

- [x] Entorno y dependencias (Conda / pip) documentadas
- [x] PreparaciÃ³n de datos (NA, split 80/20, escalado, one-hot)
- [x] Entrenamiento de 4 modelos (LogReg, SVM, DT, KNN)
- [x] SerializaciÃ³n de 4 modelos
- [x] API Flask para servir predicciones
- [x] Cliente con mÃ­nimo 2 peticiones por modelo
- [x] Repositorio pÃºblico en GitHub con README

---

## ğŸ“Œ Notas

- Ejecuta `notebooks/train_model.ipynb` **antes** de levantar `predict_app.py` para que existan los `.pck` en `models/`.
