# ğŸ§ ClasificaciÃ³n de PingÃ¼inos del ArchipiÃ©lago Palmer

Este proyecto implementa una soluciÃ³n completa de *Machine Learning* para predecir la especie de un pingÃ¼ino basÃ¡ndose en sus medidas fÃ­sicas. El flujo de trabajo abarca desde el anÃ¡lisis exploratorio de datos hasta el despliegue de modelos mediante una API REST con Flask.


## ğŸ“‹ DescripciÃ³n del Proyecto

El objetivo es clasificar pingÃ¼inos en tres especies (**Adelie**, **Chinstrap**, **Gentoo**) utilizando el dataset **Palmer Archipelago (Antarctica) penguin data**.

El proyecto incluye:

- âœ… **PreparaciÃ³n de datos**: eliminaciÃ³n de filas con `NA`, codificaciÃ³n *one-hot* con `DictVectorizer` y normalizaciÃ³n con `StandardScaler`.
- âœ… **Entrenamiento y evaluaciÃ³n** de 4 modelos: **RegresiÃ³n LogÃ­stica**, **SVM**, **Ãrbol de DecisiÃ³n**, **KNN**.
- âœ… **SerializaciÃ³n** de los modelos (`.pck`) para reutilizarlos en despliegue.
- âœ… **API REST con Flask** para realizar predicciones.
- âœ… **Cliente** para enviar al menos 2 peticiones a cada modelo y mostrar las respuestas.
---

## ğŸ“‚ Estructura del Repositorio

```plaintext
penguins-classification-flask/
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ penguins_size.csv       # Dataset original (requerido)
â”‚
â”œâ”€â”€ models/                     # Modelos entrenados (.pck)
â”‚   â”œâ”€â”€ decision_tree.pck
â”‚   â”œâ”€â”€ knn.pck
â”‚   â”œâ”€â”€ logistic_regression.pck
â”‚   â””â”€â”€ svm.pck
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ train_model.ipynb       # Notebook de limpieza, entrenamiento y serializaciÃ³n
â”‚   â””â”€â”€ client.ipynb            # Notebook para testear el cliente contra la API
â”‚
â”œâ”€â”€ predict_app.py              # Script del servidor Flask
â”œâ”€â”€ environment.yml             # Dependencias del proyecto (Conda)
â””â”€â”€ README.md                   # DocumentaciÃ³n del proyecto

```

---

## ğŸš€ InstalaciÃ³n y Requisitos

Este proyecto utiliza **Python 3.9+**. Para asegurar la compatibilidad y gestionar las dependencias correctamente, se utiliza **Conda** mediante un archivo de configuraciÃ³n `environment.yml`.

### 1. ConfiguraciÃ³n del entorno (`environment.yml`)
El archivo llamado `environment.yml` en la carpeta raÃ­z del proyecto con el siguiente contenido:

```yaml
name: penguins_env
channels:
  - defaults
dependencies:
  - python=3.9
  - pandas
  - numpy
  - scikit-learn
  - flask
  - requests
  - seaborn
  - matplotlib
  - jupyter
  - pip
```

### InstalaciÃ³n y ActivaciÃ³n

Puedes crear el entorno virtual e instalar todas las dependencias automÃ¡ticamente ejecutando los siguientes comandos en tu terminal:

```bash
# Crear el entorno a partir del archivo YAML
conda env create -f environment.yml

# Activar el entorno creado
conda activate penguins_env
```

---

## ğŸ“¦ Dataset

El proyecto requiere el archivo de datos en la siguiente ruta: `dataset/penguins_size.csv`


---

## ğŸ§ª Entrenamiento y serializaciÃ³n de modelos

Antes de iniciar el servidor, es necesario entrenar los modelos. Ejecuta paso a paso el Jupyter Notebook: `ğŸ“‚ notebooks/train_model.ipynb`

```bash
notebooks/train_model.ipynb
```

Este proceso realiza lo siguiente:

1) Carga el dataset y elimina filas con valores nulos.
2) Divide los datos **80% train / 20% test**  
3) Escala variables numÃ©ricas con `StandardScaler`
4) Aplica one-hot encoding a categÃ³ricas con `DictVectorizer`  
5) Entrena 4 modelos y los guarda en la carpeta `models/`:

- `models/logistic_regression.pck`
- `models/svm.pck`
- `models/decision_tree.pck`
- `models/knn.pck`

---

## ğŸŒ Ejecutar la API (Flask)

Una vez generados los modelos, arranca el servidor desde la terminal (estando en la raÃ­z del proyecto):

```bash
python predict_app.py
```

El servidor corre en:

- `http://127.0.0.1:9696`

### Endpoints

La API expone un endpoint por modelo:

- `POST /predict/lr` (RegresiÃ³n LogÃ­stica)
- `POST /predict/svm` (Support Vector Machine)
- `POST /predict/dt` (Ãrbol de DecisiÃ³n)
- `POST /predict/knn` (K-Nearest Neighbors)


### Ejemplo de peticiÃ³n (JSON)

```json
{
  "island": "Torgersen",
  "culmen_length_mm": 39.1,
  "culmen_depth_mm": 18.7,
  "flipper_length_mm": 181.0,
  "body_mass_g": 3750.0,
  "sex": "MALE"
}
```

### Ejemplo con `curl`

```bash
curl -X POST "http://127.0.0.1:9696/predict/lr"   -H "Content-Type: application/json"   -d '{
    "island":"Torgersen",
    "culmen_length_mm":39.1,
    "culmen_depth_mm":18.7,
    "flipper_length_mm":181.0,
    "body_mass_g":3750.0,
    "sex":"MALE"
  }'
```

---

## ğŸ§‘â€ğŸ’» Cliente de prueba

Con el servidor encendido, ejecutamos paso por paso el Jupyter Notebook::

```bash
notebooks/client.ipynb
```

El cliente enviarÃ¡ automÃ¡ticamente peticiones a cada uno de los 4 modelos y mostrarÃ¡ la predicciÃ³n en pantalla.

---

## ğŸ§  Modelos implementados

- **RegresiÃ³n logÃ­stica**: baseline lineal
- **SVM**: hiperplano separador (en el ejemplo con `probability=True`)
- **Ãrbol de decisiÃ³n**: reglas con profundidad acotada para evitar sobreajuste
- **KNN**: clasificaciÃ³n por vecinos mÃ¡s cercanos

---

## ğŸ“Š Preprocesamiento aplicado

- **Limpieza**: eliminaciÃ³n de filas con valores nulos (`dropna`).
- **Escalado**: `StandardScaler` sobre variables numÃ©ricas (media 0, desviaciÃ³n 1).  
  - Se ajusta **solo con train** y se aplica a **train y test**.
- **CodificaciÃ³n**: `DictVectorizer` para variables categÃ³ricas (`island`, `sex`) mediante one-hot.

---

## âœ… Checklist de entrega (rÃºbrica)

- [x] Entorno y dependencias (Conda / pip) documentadas
- [x] PreparaciÃ³n de datos (NA, split 80/20, escalado, one-hot)
- [x] Entrenamiento de 4 modelos (LogReg, SVM, DT, KNN)
- [x] SerializaciÃ³n de 4 modelos
- [x] API Flask para servir predicciones
- [x] Cliente con mÃ­nimo 2 peticiones por modelo
- [x] Repositorio pÃºblico en GitHub con README

---

## ğŸ“Œ Notas

- Ejecuta `notebooks/train_model.ipynb` **antes** de levantar `predict_app.py` para que existan los `.pck` en `models/`.
